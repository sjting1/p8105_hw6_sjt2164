---
title: "HW6"
output: github_document
author: Serena (sjt2164)
---

This homework uses the `tidyverse` library with `set.set(1)` for reproducibility.

```{r, include = FALSE}
library(tidyverse)
library(kableExtra)

set.seed(1)
```

## Problem 1

```{r weather}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

#Central Park data for year 2017 with info on precipitation, max and min temp
```

```{r summary, include=FALSE}
weather_df
#365 observations, 6 variables
#chr: name, id = Central Park data only
#date: date = 2017 data from Jan 1st to Dec 31st.
#dbl: prcp, tmax, tmin

summary(weather_df)
#no NAs for pcrp, tmax, tmin
```

* For each boostrap sample, interested in distribution of :
  * 𝑟̂ 2
  * log(𝛽̂ 0∗𝛽̂ 1)

```{r bootstrap}
Bootstrap = weather_df |> 
  modelr::bootstrap(n = 5000) |> #5000 bootstrap samples 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df) ), # y = tmax, x = tmin
    results = map(models, broom::tidy), #for log betas
    r_squared = map(models, broom::glance)) |> #shows R^2
   # log_betas = log(intercept * 
  unnest(results, r_squared) |> 
  janitor::clean_names() |>
  select(id, term, estimate, r_squared) |>
   mutate(
    term = ifelse(term == "(Intercept)", "intercept", term)) |>
  pivot_wider(
    names_from = "term", 
    values_from = "estimate") |>
  mutate(
    log_beta0xbeta1 = log(intercept * tmin)) |> #log of the product of beta0 and beta1
  select(id, r_squared, log_beta0xbeta1)

Bootstrap
```

##### Distribution of 𝑟̂ 2 estimates

```{r estimate plots}
Bootstrap |>
  ggplot(aes(x = id, y = r_squared)) + 
  geom_point(alpha = 0.5) +
  stat_smooth(method = "lm") +
  labs(title = "R squared distribution for 5000 bootstrap samples",
       x = "Boostrap Sample Id",
       y = "R squared") 
  
```

##### Distribution of log(𝛽̂ 0∗𝛽̂ 1) estimates

```{r}
Bootstrap |>
  ggplot(aes(x = id, y = log_beta0xbeta1)) + 
  geom_point(alpha = 0.5) +
  stat_smooth(method = "lm") +
  labs(title = "Log(beta0 * beta1) distribution for 5000 bootstrap samples",
       x = "Boostrap Sample Id",
       y = "Log(beta0 * beta1") 

```

Both plots indicate an uniformly distribution of the estimates. The distribution of the R squared estimates has a higher density around 0.90 and 0.92. In comparison, the  log(𝛽̂ 0∗𝛽̂ distribution is more concentrated between 2.00 and 2.05. 
       
       
##### Confidence Intervals

```{r}
CI_estimates = Bootstrap |> 
  summarize(
    R2_ci_lower = quantile(r_squared, 0.025), 
    R2_ci_upper = quantile(r_squared, 0.975),
    logBetas_ci_lower = quantile(log_beta0xbeta1, 0.025), 
    logBetas_ci_upper = quantile(log_beta0xbeta1, 0.975)) 

CI_estimates
```


Using the 5000 bootstrap estimates, we find that the 95% confidence interval for 𝑟̂ 2 is (0.894, 0.927).

The 95% confidence interval for log(𝛽̂ 0∗𝛽̂ 1) is (1.964, 2.059).


## Problem 2: Homicides in 50 large U.S. cities 

```{r Homicide data}
homicide = read_csv("./data/homicide-data.csv", na = c("NA", ".", "")) |>
  janitor::clean_names()

#52,179 observations and 12 variables
#victim_age has numeric values and some "unknown"

summary(homicide)
#char = uid, victim_last, victim_first, victim_race, victim_age, victim_sex, city, state, and disposition
#dbl = reported_date, lat, lon

homicide |> distinct(victim_race) #Hispanic, White, Other, Black, Asian, and Unknown
homicide |> distinct(victim_sex) #male, female, unknown
homicide |> distinct(victim_age) #102 distinct ages, including unknown 
homicide |> distinct(city) #50
homicide |> distinct(state) #28 states in abbrev. (wisconsin = "wI"?)

#unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

homicide |> count(disposition) #3 categories: closed without arrest (2922), closed by arrest (25674), open/no arrest (23583). Expect unsolved = 26505

#sorting reported_date: 2 dates have an extra number (201511105, 201511018)
```

**Raw Data**
The raw data `homicide` has `r nrow(homicide)` observations and `r ncol(homicide)` variables.

* Identity variables include 
  * `uid` (`r count(distinct(homicide, uid))`), 
  * Name of victim (`victim_first` and `victim_last`)
  * `victim_race` (Hispanic, White, Other, Black, Asian, and Unknown), 
  * `victim_age` (includes Unknown)
  * `victim_sex` (Male, Female, Unknown)
* Date variable includes `reported_date` in the format YYYYMMDD. 2 entries does not follow this format (has an extra number).
* Location variables include `city` (50), `state`, latitude (`lat`), and longitude (`lon`). 
* The last variable is `disposition` (`r count(distinct(homicide, disposition))` categories)

**Data Cleaning**

* Fixed two `reported_date` entries that had an extra number
* Fixed abbreviation for Wisconsin
* created new `city_state` variable
* Created a `city_state` variable (e.g. “Baltimore, MD”)
* Created a binary variable `homicide_solved` indicating whether the homicide is solved (yes vs. no)
* Ensured `victim_age` is numeric
* Omited cities Dallas, TX; Phoenix, AZ; and Kansas City, MO (they don’t report victim race) 
* Omited Tulsa, AL (is a data entry mistake)
* Limit your analysis to those for whom `victim_race` is `white` or `black`. 


```{r fix}
homicide_data = homicide |>
  mutate(
    reported_date = case_match(reported_date,
      201511105 ~ 20151105,
      201511018 ~ 20151018,
      .default = reported_date),
    reported_date = as.Date.character(reported_date, format = "%Y%m%d"),
     state = case_match(state,
      'wI' ~ 'WI',
      .default = state),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex),
    victim_age = as.numeric(victim_age),
    city = as.factor(city),
    state = as.factor(state),
    disposition = as.factor(disposition)) |>
  mutate(
    city_state = paste(city, state, sep = ", "), #create city_state var
    homicide_solved = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1)) |> # 0 = unresolved, 1 = resolved
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"), #48507
    victim_race %in% c("White", "Black"))
  
#Formatted date and checked so that:  
#GERALD A. BUNCH: 201511105 -> 20151105 -> 2015-11-05 
#LUIS SALAS: 201511018 -> 20151018 ->2015-10-18
#wisconsin = WI

#39693 observations
```

_*_Baltimore, MD_

* For the city of Baltimore, MD, the `glm` function is used to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 
  * Output of `glm` is saved as a csv file (Baltimore_glm.csv) and `broom::tidy` was applied to this object

```{r baltimore}

baltimore_df = homicide_data |>
  filter(city_state == "Baltimore, MD") |>
  mutate(
    victim_race = fct_relevel(victim_race, "White"), #reference = White
    victim_sex = fct_relevel(victim_sex, "Male")) |>  #reference = Male
  select(homicide_solved, victim_age, victim_sex, victim_race)

fit_logistic = 
  baltimore_df |> 
  glm(homicide_solved ~ victim_age + victim_sex + victim_race , data = _, family = binomial()) 

glm_output = fit_logistic |> 
  broom::tidy() |> 
  mutate( 
    OR = exp(estimate), #obtain estimate
    CI_lower = exp(estimate - (1.96*std.error)) , #lower CI
    CI_upper = exp(estimate + (1.96*std.error)), #upper CI
    ) |>
  select(term, OR, CI_lower, CI_upper) |>
  knitr::kable(digits = 3)

 #output of glm saved as csv
print(glm_output)
write.csv(glm_output, "Baltimore_glm.csv")

#95% CI = estimate ± (critical value * standard error)
```

**Solving homicides comparing male victims to female victims keeping all other variables fixed**

Based on the output, the OR estimate for solved homicides comparing female vs. male is 2.35 and its 95% confidence interval is (1.793, 3.081), adjusting for age and race. 

The odds of solving homicides for females is 2.35 times higher than the odds of solving homicides for males. We are 95% confident that the true odds of solving homicides comparing males to females is between 1.793 and 3.081. (Reference = Male, White)

_Glm for the Cities_

```{r city glm}

homicide_data |> count(victim_sex) #41 unknown sex

#set reference = White and Male
city_df = homicide_data |>
  filter(
    victim_sex %in% c("Male", "Female")) |> #keep only male and female
  mutate(
    victim_race = fct_relevel(victim_race, "White"), 
    victim_sex = fct_relevel(victim_sex, "Male")) |>
  select(city_state, homicide_solved, victim_age, victim_sex, victim_race) |>
    group_by(city_state) |>
  nest() |>
  mutate(
    city_model = map(data, ~glm(homicide_solved ~ victim_age + victim_sex + victim_race , data = ., family = binomial())),
    city_results = map(city_model, broom::tidy)) |>
  unnest(city_results) |> 
  mutate( 
    OR = exp(estimate), #obtain estimate
    CI_lower = exp(estimate - (1.96*std.error)) , #lower CI
    CI_upper = exp(estimate + (1.96*std.error)), #upper CI
    ) |>
  select(city_state, term, OR, CI_lower, CI_upper) |>
  filter(
    term == "victim_sexFemale") |>
   rename(OR_sex = OR) |>
  select(-term) 

knitr::kable(city_df, digits = 3)

city_df |> distinct(city_state) #47 city_states

```

* Ran glm for each of the cities in your dataset
* Extracted the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims.

*Plot*

```{r plot 1}
city_plot = city_df |>
  ggplot(aes(x = reorder(city_state, OR_sex), y = OR_sex)) + 
  geom_point() +  
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Solved Homicides Female vs. Male Across 47 Cities",
  x = "Location", 
  y = "Odds Ratio with CI")
  
city_plot
```

The plot shows the estimated ORs and CIs for each city with cities organized by estimated OR.
Albuquerque, NM has the smallest OR and New York, NY has the highest. New York, NY also has a wide confidence interval, which is seen in San Bernardino, CA and Long Beach, CA as well. 

## Problem 3: Effects of variables on a child’s birthweight

```{r bwt}

bwt_df = read_csv("./data/birthweight.csv", na = c("NA", ".", ""))

#4342 observations, 20 variables

```

Dataset has ~4000 children and includes the following variables:

* `babysex`: baby’s sex (male = 1, female = 2)
* `bhead`: baby’s head circumference at birth (centimeters)
* `blength`: baby’s length at birth (centimeteres)
* `bwt`: baby’s birth weight (grams)
* `delwt`: mother’s weight at delivery (pounds)
* `fincome`: family monthly income (in hundreds, rounded)
* `frace`: father’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
* `gaweeks`: gestational age in weeks
* `malform`: presence of malformations that could affect weight (0 = absent, 1 = present)
* `menarche`: mother’s age at menarche (years)
* `mheigth`: mother’s height (inches)
* `momage`: mother’s age at delivery (years)
* `mrace`: mother’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)
* `parity`: number of live births prior to this pregnancy
* `pnumlbw`: previous number of low birth weight babies
* `pnumgsa`: number of prior small for gestational age babies
* `ppbmi`: mother’s pre-pregnancy BMI
* `ppwt`: mother’s pre-pregnancy weight (pounds)
* `smoken`: average number of cigarettes smoked per day during pregnancy
* wtgain`: mother’s weight gain during pregnancy (pounds)

#### Data Cleaning
```{r bwt clean}
#clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).
```

#### Regression Model

```{r Model}

```

Propose a regression model for birthweight. 
* Model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. 
* Describe modeling process

```{r plot 2}

```

* Show a plot of model residuals against fitted values – use `add_predictions` and `add_residuals` in making this plot.

#### Compare your model to two others:

* One using length at birth and gestational age as predictors (main effects only)

* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

Make this comparison in terms of the cross-validated prediction error; use `crossv_mc` and functions in `purrr` as appropriate.

Note that although we expect your model to be reasonable, model building itself is not a main idea of the course and we don’t necessarily expect your model to be “optimal”.

