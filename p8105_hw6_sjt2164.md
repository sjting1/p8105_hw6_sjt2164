HW6
================
Serena (sjt2164)

This homework uses the `tidyverse` library with `set.set(1)` for
reproducibility.

## Problem 1

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## using cached file: /Users/serenating/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2024-10-27 21:40:52.853389 (8.656)

    ## file min/max dates: 1869-01-01 / 2024-10-31

``` r
#Central Park data for year 2017 with info on precipitation, max and min temp
```

- For each boostrap sample, interested in distribution of :
  - ğ‘ŸÌ‚ 2
  - log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ 1)

``` r
Bootstrap = weather_df |> 
  modelr::bootstrap(n = 5000) |> #5000 bootstrap samples 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df) ), # y = tmax, x = tmin
    results = map(models, broom::tidy), #for log betas
    r_squared = map(models, broom::glance)) |> #shows R^2
   # log_betas = log(intercept * 
  unnest(results, r_squared) |> 
  janitor::clean_names() |>
  select(id, term, estimate, r_squared) |>
   mutate(
    term = ifelse(term == "(Intercept)", "intercept", term)) |>
  pivot_wider(
    names_from = "term", 
    values_from = "estimate") |>
  mutate(
    log_beta0xbeta1 = log(intercept * tmin)) |> #log of the product of beta0 and beta1
  select(id, r_squared, log_beta0xbeta1)
```

    ## Warning: `unnest()` has a new interface. See `?unnest` for details.
    ## â„¹ Try `df %>% unnest(c(results, r_squared))`, with `mutate()` if needed.

``` r
Bootstrap
```

    ## # A tibble: 5,000 Ã— 3
    ##    id    r_squared log_beta0xbeta1
    ##    <chr>     <dbl>           <dbl>
    ##  1 0001      0.907            2.04
    ##  2 0002      0.896            2.03
    ##  3 0003      0.918            2.05
    ##  4 0004      0.899            2.07
    ##  5 0005      0.913            1.97
    ##  6 0006      0.919            1.99
    ##  7 0007      0.918            2.00
    ##  8 0008      0.907            2.00
    ##  9 0009      0.916            2.01
    ## 10 0010      0.918            2.02
    ## # â„¹ 4,990 more rows

##### Distribution of ğ‘ŸÌ‚ 2 estimates

``` r
Bootstrap |>
  ggplot(aes(x = id, y = r_squared)) + 
  geom_point(alpha = 0.5) +
  stat_smooth(method = "lm") +
  labs(title = "R squared distribution for 5000 bootstrap samples",
       x = "Boostrap Sample Id",
       y = "R squared") 
```

    ## `geom_smooth()` using formula = 'y ~ x'

![](p8105_hw6_sjt2164_files/figure-gfm/estimate%20plots-1.png)<!-- -->

##### Distribution of log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ 1) estimates

``` r
Bootstrap |>
  ggplot(aes(x = id, y = log_beta0xbeta1)) + 
  geom_point(alpha = 0.5) +
  stat_smooth(method = "lm") +
  labs(title = "Log(beta0 * beta1) distribution for 5000 bootstrap samples",
       x = "Boostrap Sample Id",
       y = "Log(beta0 * beta1") 
```

    ## `geom_smooth()` using formula = 'y ~ x'

![](p8105_hw6_sjt2164_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

Both plots indicate an uniformly distribution of the estimates. The
distribution of the R squared estimates has a higher density around 0.90
and 0.92. In comparison, the log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ distribution is more concentrated
between 2.00 and 2.05.

##### Confidence Intervals

``` r
CI_estimates = Bootstrap |> 
  summarize(
    R2_ci_lower = quantile(r_squared, 0.025), 
    R2_ci_upper = quantile(r_squared, 0.975),
    logBetas_ci_lower = quantile(log_beta0xbeta1, 0.025), 
    logBetas_ci_upper = quantile(log_beta0xbeta1, 0.975)) 

CI_estimates
```

    ## # A tibble: 1 Ã— 4
    ##   R2_ci_lower R2_ci_upper logBetas_ci_lower logBetas_ci_upper
    ##         <dbl>       <dbl>             <dbl>             <dbl>
    ## 1       0.894       0.927              1.96              2.06

Using the 5000 bootstrap estimates, we find that the 95% confidence
interval for ğ‘ŸÌ‚ 2 is (0.894, 0.927).

The 95% confidence interval for log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ 1) is (1.964, 2.059).

## Problem 2: Homicides in 50 large U.S. cities

``` r
#Import from github repo, read article.
```

- Create a `city_state` variable (e.g.Â â€œBaltimore, MDâ€), and a binary
  variable indicating whether the homicide is solved (ex. `solved`: y/n)

- Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO (they donâ€™t
  report victim race)

- Omit Tulsa, AL (is a data entry mistake)

- Limit your analysis to those for whom `victim_race` is `white` or
  `black`.

- Be sure that `victim_age` is numeric.

\_\**Baltimore, MD*

- For the city of Baltimore, MD, use the `glm` function to fit a
  logistic regression with resolved vs unresolved as the outcome and
  victim age, sex and race as predictors.
- Save output of `glm` as an R object; apply the `broom::tidy` to this
  object
- Obtain the estimate and confidence interval of the adjusted odds ratio
  for solving homicides comparing male victims to female victims keeping
  all other variables fixed.

*Glm for the Cities*

- Now run glm for each of the cities in your dataset
- Extract the adjusted odds ratio (and CI) for solving homicides
  comparing male victims to female victims.
- Do this within a â€œtidyâ€ pipeline, making use of `purrr::map`, list
  columns, and `unnest` as necessary to create a dataframe with
  estimated ORs and CIs for each city.

*Plot*

- Shows the estimated ORs and CIs for each city on plot
- Organize cities according to estimated OR
- Comment on the plot

## Problem 3: Effects of variables on a childâ€™s birthweight

``` r
bwt_df = read_csv("./data/birthweight.csv", na = c("NA", ".", ""))
```

    ## Rows: 4342 Columns: 20
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#4342 observations, 20 variables
```

Dataset has ~4000 children and includes the following variables:

- `babysex`: babyâ€™s sex (male = 1, female = 2)
- `bhead`: babyâ€™s head circumference at birth (centimeters)
- `blength`: babyâ€™s length at birth (centimeteres)
- `bwt`: babyâ€™s birth weight (grams)
- `delwt`: motherâ€™s weight at delivery (pounds)
- `fincome`: family monthly income (in hundreds, rounded)
- `frace`: fatherâ€™s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto
  Rican, 8 = Other, 9 = Unknown)
- `gaweeks`: gestational age in weeks
- `malform`: presence of malformations that could affect weight (0 =
  absent, 1 = present)
- `menarche`: motherâ€™s age at menarche (years)
- `mheigth`: motherâ€™s height (inches)
- `momage`: motherâ€™s age at delivery (years)
- `mrace`: motherâ€™s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto
  Rican, 8 = Other)
- `parity`: number of live births prior to this pregnancy
- `pnumlbw`: previous number of low birth weight babies
- `pnumgsa`: number of prior small for gestational age babies
- `ppbmi`: motherâ€™s pre-pregnancy BMI
- `ppwt`: motherâ€™s pre-pregnancy weight (pounds)
- `smoken`: average number of cigarettes smoked per day during pregnancy
- wtgain\`: motherâ€™s weight gain during pregnancy (pounds)

#### Data Cleaning

``` r
#clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).
```

#### Regression Model

Propose a regression model for birthweight. \* Model may be based on a
hypothesized structure for the factors that underly birthweight, on a
data-driven model-building process, or a combination of the two. \*
Describe modeling process

- Show a plot of model residuals against fitted values â€“ use
  `add_predictions` and `add_residuals` in making this plot.

#### Compare your model to two others:

- One using length at birth and gestational age as predictors (main
  effects only)

- One using head circumference, length, sex, and all interactions
  (including the three-way interaction) between these

Make this comparison in terms of the cross-validated prediction error;
use `crossv_mc` and functions in `purrr` as appropriate.

Note that although we expect your model to be reasonable, model building
itself is not a main idea of the course and we donâ€™t necessarily expect
your model to be â€œoptimalâ€.
