HW6
================
Serena (sjt2164)

This homework uses the `tidyverse` library with `set.set(1)` for
reproducibility.

## Problem 1

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## using cached file: /Users/serenating/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2024-10-27 21:40:52.853389 (8.656)

    ## file min/max dates: 1869-01-01 / 2024-10-31

``` r
#Central Park data for year 2017 with info on precipitation, max and min temp
```

- For each boostrap sample, interested in distribution of :
  - ğ‘ŸÌ‚ 2
  - log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ 1)

``` r
Bootstrap = weather_df |> 
  modelr::bootstrap(n = 5000) |> #5000 bootstrap samples 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df) ), # y = tmax, x = tmin
    results = map(models, broom::tidy), #for log betas
    r_squared = map(models, broom::glance)) |> #shows R^2
   # log_betas = log(intercept * 
  unnest(results, r_squared) |> 
  janitor::clean_names() |>
  select(id, term, estimate, r_squared) |>
   mutate(
    term = ifelse(term == "(Intercept)", "intercept", term)) |>
  pivot_wider(
    names_from = "term", 
    values_from = "estimate") |>
  mutate(
    log_beta0xbeta1 = log(intercept * tmin)) |> #log of the product of beta0 and beta1
  select(id, r_squared, log_beta0xbeta1)
```

    ## Warning: `unnest()` has a new interface. See `?unnest` for details.
    ## â„¹ Try `df %>% unnest(c(results, r_squared))`, with `mutate()` if needed.

``` r
Bootstrap
```

    ## # A tibble: 5,000 Ã— 3
    ##    id    r_squared log_beta0xbeta1
    ##    <chr>     <dbl>           <dbl>
    ##  1 0001      0.907            2.04
    ##  2 0002      0.896            2.03
    ##  3 0003      0.918            2.05
    ##  4 0004      0.899            2.07
    ##  5 0005      0.913            1.97
    ##  6 0006      0.919            1.99
    ##  7 0007      0.918            2.00
    ##  8 0008      0.907            2.00
    ##  9 0009      0.916            2.01
    ## 10 0010      0.918            2.02
    ## # â„¹ 4,990 more rows

##### Distribution of ğ‘ŸÌ‚ 2 estimates

``` r
Bootstrap |>
  ggplot(aes(x = id, y = r_squared)) + 
  geom_point(alpha = 0.5) +
  stat_smooth(method = "lm") +
  labs(title = "R squared distribution for 5000 bootstrap samples",
       x = "Boostrap Sample Id",
       y = "R squared") 
```

    ## `geom_smooth()` using formula = 'y ~ x'

![](p8105_hw6_sjt2164_files/figure-gfm/estimate%20plots-1.png)<!-- -->

##### Distribution of log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ 1) estimates

``` r
Bootstrap |>
  ggplot(aes(x = id, y = log_beta0xbeta1)) + 
  geom_point(alpha = 0.5) +
  stat_smooth(method = "lm") +
  labs(title = "Log(beta0 * beta1) distribution for 5000 bootstrap samples",
       x = "Boostrap Sample Id",
       y = "Log(beta0 * beta1") 
```

    ## `geom_smooth()` using formula = 'y ~ x'

![](p8105_hw6_sjt2164_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

Both plots indicate an uniformly distribution of the estimates. The
distribution of the R squared estimates has a higher density around 0.90
and 0.92. In comparison, the log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ distribution is more concentrated
between 2.00 and 2.05.

##### Confidence Intervals

``` r
CI_estimates = Bootstrap |> 
  summarize(
    R2_ci_lower = quantile(r_squared, 0.025), 
    R2_ci_upper = quantile(r_squared, 0.975),
    logBetas_ci_lower = quantile(log_beta0xbeta1, 0.025), 
    logBetas_ci_upper = quantile(log_beta0xbeta1, 0.975)) 

CI_estimates
```

    ## # A tibble: 1 Ã— 4
    ##   R2_ci_lower R2_ci_upper logBetas_ci_lower logBetas_ci_upper
    ##         <dbl>       <dbl>             <dbl>             <dbl>
    ## 1       0.894       0.927              1.96              2.06

Using the 5000 bootstrap estimates, we find that the 95% confidence
interval for ğ‘ŸÌ‚ 2 is (0.894, 0.927).

The 95% confidence interval for log(ğ›½Ì‚ 0âˆ—ğ›½Ì‚ 1) is (1.964, 2.059).

## Problem 2: Homicides in 50 large U.S. cities

``` r
homicide = read_csv("./data/homicide-data.csv", na = c("NA", ".", "")) |>
  janitor::clean_names()
```

    ## Rows: 52179 Columns: 12
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#52,179 observations and 12 variables
#victim_age has numeric values and some "unknown"

summary(homicide)
```

    ##      uid            reported_date       victim_last        victim_first      
    ##  Length:52179       Min.   : 20070101   Length:52179       Length:52179      
    ##  Class :character   1st Qu.: 20100318   Class :character   Class :character  
    ##  Mode  :character   Median : 20121216   Mode  :character   Mode  :character  
    ##                     Mean   : 20130899                                        
    ##                     3rd Qu.: 20150911                                        
    ##                     Max.   :201511105                                        
    ##                                                                              
    ##  victim_race         victim_age         victim_sex            city          
    ##  Length:52179       Length:52179       Length:52179       Length:52179      
    ##  Class :character   Class :character   Class :character   Class :character  
    ##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##     state                lat             lon          disposition       
    ##  Length:52179       Min.   :25.73   Min.   :-122.51   Length:52179      
    ##  Class :character   1st Qu.:33.77   1st Qu.: -96.00   Class :character  
    ##  Mode  :character   Median :38.52   Median : -87.71   Mode  :character  
    ##                     Mean   :37.03   Mean   : -91.47                     
    ##                     3rd Qu.:40.03   3rd Qu.: -81.76                     
    ##                     Max.   :45.05   Max.   : -71.01                     
    ##                     NA's   :60      NA's   :60

``` r
#char = uid, victim_last, victim_first, victim_race, victim_age, victim_sex, city, state, and disposition
#dbl = reported_date, lat, lon

homicide |> distinct(victim_race) #Hispanic, White, Other, Black, Asian, and Unknown
```

    ## # A tibble: 6 Ã— 1
    ##   victim_race
    ##   <chr>      
    ## 1 Hispanic   
    ## 2 White      
    ## 3 Other      
    ## 4 Black      
    ## 5 Asian      
    ## 6 Unknown

``` r
homicide |> distinct(victim_sex) #male, female, unknown
```

    ## # A tibble: 3 Ã— 1
    ##   victim_sex
    ##   <chr>     
    ## 1 Male      
    ## 2 Female    
    ## 3 Unknown

``` r
homicide |> distinct(victim_age) #102 distinct ages, including unknown 
```

    ## # A tibble: 102 Ã— 1
    ##    victim_age
    ##    <chr>     
    ##  1 78        
    ##  2 17        
    ##  3 15        
    ##  4 32        
    ##  5 72        
    ##  6 91        
    ##  7 52        
    ##  8 56        
    ##  9 43        
    ## 10 20        
    ## # â„¹ 92 more rows

``` r
homicide |> distinct(city) #50
```

    ## # A tibble: 50 Ã— 1
    ##    city       
    ##    <chr>      
    ##  1 Albuquerque
    ##  2 Atlanta    
    ##  3 Baltimore  
    ##  4 Baton Rouge
    ##  5 Birmingham 
    ##  6 Boston     
    ##  7 Buffalo    
    ##  8 Charlotte  
    ##  9 Chicago    
    ## 10 Cincinnati 
    ## # â„¹ 40 more rows

``` r
homicide |> distinct(state) #28 states in abbrev. (wisconsin = "wI"?)
```

    ## # A tibble: 28 Ã— 1
    ##    state
    ##    <chr>
    ##  1 NM   
    ##  2 GA   
    ##  3 MD   
    ##  4 LA   
    ##  5 AL   
    ##  6 MA   
    ##  7 NY   
    ##  8 NC   
    ##  9 IL   
    ## 10 OH   
    ## # â„¹ 18 more rows

``` r
#unsolved homicides (those for which the disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).

homicide |> count(disposition) #3 categories: closed without arrest (2922), closed by arrest (25674), open/no arrest (23583). Expect unsolved = 26505
```

    ## # A tibble: 3 Ã— 2
    ##   disposition               n
    ##   <chr>                 <int>
    ## 1 Closed by arrest      25674
    ## 2 Closed without arrest  2922
    ## 3 Open/No arrest        23583

``` r
#sorting reported_date: 2 dates have an extra number (201511105, 201511018)
```

**Raw Data** The raw data `homicide` has 52179 observations and 12
variables.

- Identity variables include
  - `uid` (52179),
  - Name of victim (`victim_first` and `victim_last`)
  - `victim_race` (Hispanic, White, Other, Black, Asian, and Unknown),
  - `victim_age` (includes Unknown)
  - `victim_sex` (Male, Female, Unknown)
- Date variable includes `reported_date` in the format YYYYMMDD. 2
  entries does not follow this format (has an extra number).
- Location variables include `city` (50), `state`, latitude (`lat`), and
  longitude (`lon`).
- The last variable is `disposition` (3 categories)

**Data Cleaning**

- Fixed two `reported_date` entries that had an extra number
- Fixed abbreviation for Wisconsin
- created new `city_state` variable
- Created a `city_state` variable (e.g.Â â€œBaltimore, MDâ€)
- Created a binary variable `homicide_solved` indicating whether the
  homicide is solved (yes vs.Â no)
- Ensured `victim_age` is numeric
- Omited cities Dallas, TX; Phoenix, AZ; and Kansas City, MO (they donâ€™t
  report victim race)
- Omited Tulsa, AL (is a data entry mistake)
- Limit your analysis to those for whom `victim_race` is `white` or
  `black`.

``` r
homicide_data = homicide |>
  mutate(
    reported_date = case_match(reported_date,
      201511105 ~ 20151105,
      201511018 ~ 20151018,
      .default = reported_date),
    reported_date = as.Date.character(reported_date, format = "%Y%m%d"),
     state = case_match(state,
      'wI' ~ 'WI',
      .default = state),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex),
    victim_age = as.numeric(victim_age),
    city = as.factor(city),
    state = as.factor(state),
    disposition = as.factor(disposition)) |>
  mutate(
    city_state = paste(city, state, sep = ", "), #create city_state var
    homicide_solved = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1)) |> # 0 = unresolved, 1 = resolved
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"), #48507
    victim_race %in% c("White", "Black"))
```

    ## Warning: There was 1 warning in `mutate()`.
    ## â„¹ In argument: `victim_age = as.numeric(victim_age)`.
    ## Caused by warning:
    ## ! NAs introduced by coercion

``` r
#Formatted date and checked so that:  
#GERALD A. BUNCH: 201511105 -> 20151105 -> 2015-11-05 
#LUIS SALAS: 201511018 -> 20151018 ->2015-10-18
#wisconsin = WI

#39693 observations
```

\_\**Baltimore, MD*

- For the city of Baltimore, MD, the `glm` function is used to fit a
  logistic regression with resolved vs unresolved as the outcome and
  victim age, sex and race as predictors.
  - Output of `glm` is saved as a csv file (Baltimore_glm.csv) and
    `broom::tidy` was applied to this object

``` r
baltimore_df = homicide_data |>
  filter(city_state == "Baltimore, MD") |>
  mutate(
    victim_race = fct_relevel(victim_race, "White"), #reference = White
    victim_sex = fct_relevel(victim_sex, "Male")) |>  #reference = Male
  select(homicide_solved, victim_age, victim_sex, victim_race)

fit_logistic = 
  baltimore_df |> 
  glm(homicide_solved ~ victim_age + victim_sex + victim_race , data = _, family = binomial()) 

glm_output = fit_logistic |> 
  broom::tidy() |> 
  mutate( 
    OR = exp(estimate), #obtain estimate
    CI_lower = exp(estimate - (1.96*std.error)) , #lower CI
    CI_upper = exp(estimate + (1.96*std.error)), #upper CI
    ) |>
  select(term, OR, CI_lower, CI_upper) |>
  knitr::kable(digits = 3)

 #output of glm saved as csv
print(glm_output)
```

    ## 
    ## 
    ## |term             |    OR| CI_lower| CI_upper|
    ## |:----------------|-----:|--------:|--------:|
    ## |(Intercept)      | 1.346|    0.888|    2.041|
    ## |victim_age       | 0.993|    0.987|    1.000|
    ## |victim_sexFemale | 2.350|    1.793|    3.081|
    ## |victim_raceBlack | 0.431|    0.306|    0.607|

``` r
write.csv(glm_output, "Baltimore_glm.csv")

#95% CI = estimate Â± (critical value * standard error)
```

**Solving homicides comparing male victims to female victims keeping all
other variables fixed**

Based on the output, the OR estimate for solved homicides comparing
female vs.Â male is 2.35 and its 95% confidence interval is (1.793,
3.081), adjusting for age and race.

The odds of solving homicides for females is 2.35 times higher than the
odds of solving homicides for males. We are 95% confident that the true
odds of solving homicides comparing males to females is between 1.793
and 3.081. (Reference = Male, White)

*Glm for the Cities*

``` r
homicide_data |> count(victim_sex) #41 unknown sex
```

    ## # A tibble: 3 Ã— 2
    ##   victim_sex     n
    ##   <fct>      <int>
    ## 1 Female      5903
    ## 2 Male       33749
    ## 3 Unknown       41

``` r
#set reference = White and Male
city_df = homicide_data |>
  filter(
    victim_sex %in% c("Male", "Female")) |> #keep only male and female
  mutate(
    victim_race = fct_relevel(victim_race, "White"), 
    victim_sex = fct_relevel(victim_sex, "Male")) |>
  select(city_state, homicide_solved, victim_age, victim_sex, victim_race) |>
    group_by(city_state) |>
  nest() |>
  mutate(
    city_model = map(data, ~glm(homicide_solved ~ victim_age + victim_sex + victim_race , data = ., family = binomial())),
    city_results = map(city_model, broom::tidy)) |>
  unnest(city_results) |> 
  mutate( 
    OR = exp(estimate), #obtain estimate
    CI_lower = exp(estimate - (1.96*std.error)) , #lower CI
    CI_upper = exp(estimate + (1.96*std.error)), #upper CI
    ) |>
  select(city_state, term, OR, CI_lower, CI_upper) |>
  filter(
    term == "victim_sexFemale") |>
   rename(OR_sex = OR) |>
  select(-term) 

knitr::kable(city_df, digits = 3)
```

| city_state         | OR_sex | CI_lower | CI_upper |
|:-------------------|-------:|---------:|---------:|
| Albuquerque, NM    |  0.566 |    0.266 |    1.204 |
| Atlanta, GA        |  1.000 |    0.683 |    1.463 |
| Baltimore, MD      |  2.350 |    1.793 |    3.081 |
| Baton Rouge, LA    |  2.622 |    1.438 |    4.779 |
| Birmingham, AL     |  1.149 |    0.759 |    1.741 |
| Boston, MA         |  1.499 |    0.794 |    2.829 |
| Buffalo, NY        |  1.921 |    1.069 |    3.451 |
| Charlotte, NC      |  1.131 |    0.713 |    1.795 |
| Chicago, IL        |  2.438 |    1.998 |    2.976 |
| Cincinnati, OH     |  2.501 |    1.477 |    4.236 |
| Columbus, OH       |  1.878 |    1.334 |    2.644 |
| Denver, CO         |  2.087 |    1.030 |    4.230 |
| Detroit, MI        |  1.717 |    1.363 |    2.164 |
| Durham, NC         |  1.231 |    0.594 |    2.551 |
| Fort Worth, TX     |  1.495 |    0.887 |    2.519 |
| Fresno, CA         |  0.749 |    0.326 |    1.723 |
| Houston, TX        |  1.406 |    1.103 |    1.793 |
| Indianapolis, IN   |  1.088 |    0.805 |    1.472 |
| Jacksonville, FL   |  1.389 |    1.036 |    1.864 |
| Las Vegas, NV      |  1.194 |    0.867 |    1.646 |
| Long Beach, CA     |  2.438 |    0.924 |    6.430 |
| Los Angeles, CA    |  1.511 |    1.046 |    2.183 |
| Louisville, KY     |  2.039 |    1.266 |    3.282 |
| Memphis, TN        |  1.383 |    1.012 |    1.890 |
| Miami, FL          |  1.941 |    1.147 |    3.284 |
| Milwaukee, WI      |  1.375 |    0.943 |    2.005 |
| Minneapolis, MN    |  1.056 |    0.533 |    2.091 |
| Nashville, TN      |  0.967 |    0.640 |    1.460 |
| New Orleans, LA    |  1.710 |    1.233 |    2.371 |
| New York, NY       |  3.811 |    2.003 |    7.249 |
| Oakland, CA        |  1.776 |    1.151 |    2.739 |
| Oklahoma City, OK  |  1.027 |    0.658 |    1.602 |
| Omaha, NE          |  2.614 |    1.387 |    4.927 |
| Philadelphia, PA   |  2.015 |    1.533 |    2.648 |
| Pittsburgh, PA     |  2.322 |    1.429 |    3.772 |
| Richmond, VA       |  0.994 |    0.492 |    2.008 |
| San Antonio, TX    |  1.419 |    0.801 |    2.515 |
| Sacramento, CA     |  1.495 |    0.748 |    2.988 |
| Savannah, GA       |  1.153 |    0.562 |    2.368 |
| San Bernardino, CA |  1.999 |    0.684 |    5.841 |
| San Diego, CA      |  2.421 |    1.170 |    5.012 |
| San Francisco, CA  |  1.646 |    0.858 |    3.157 |
| St.Â Louis, MO      |  1.422 |    1.073 |    1.885 |
| Stockton, CA       |  0.740 |    0.340 |    1.610 |
| Tampa, FL          |  1.238 |    0.533 |    2.876 |
| Tulsa, OK          |  1.025 |    0.644 |    1.630 |
| Washington, DC     |  1.447 |    0.982 |    2.132 |

``` r
city_df |> distinct(city_state) #47 city_states
```

    ## # A tibble: 47 Ã— 1
    ## # Groups:   city_state [47]
    ##    city_state     
    ##    <chr>          
    ##  1 Albuquerque, NM
    ##  2 Atlanta, GA    
    ##  3 Baltimore, MD  
    ##  4 Baton Rouge, LA
    ##  5 Birmingham, AL 
    ##  6 Boston, MA     
    ##  7 Buffalo, NY    
    ##  8 Charlotte, NC  
    ##  9 Chicago, IL    
    ## 10 Cincinnati, OH 
    ## # â„¹ 37 more rows

- Ran glm for each of the cities in your dataset
- Extracted the adjusted odds ratio (and CI) for solving homicides
  comparing male victims to female victims.

*Plot*

``` r
city_plot = city_df |>
  ggplot(aes(x = reorder(city_state, OR_sex), y = OR_sex)) + 
  geom_point() +  
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Solved Homicides Female vs. Male Across 47 Cities",
  x = "Location", 
  y = "Odds Ratio with CI")
  
city_plot
```

![](p8105_hw6_sjt2164_files/figure-gfm/plot%201-1.png)<!-- -->

The plot shows the estimated ORs and CIs for each city with cities
organized by estimated OR. Albuquerque, NM has the smallest OR and New
York, NY has the highest. New York, NY also has a wide confidence
interval, which is seen in San Bernardino, CA and Long Beach, CA as
well.

## Problem 3: Effects of variables on a childâ€™s birthweight

``` r
bwt_df = read_csv("./data/birthweight.csv", na = c("NA", ".", ""))
```

    ## Rows: 4342 Columns: 20
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#4342 observations, 20 variables
```

Dataset has ~4000 children and includes the following variables:

- `babysex`: babyâ€™s sex (male = 1, female = 2)
- `bhead`: babyâ€™s head circumference at birth (centimeters)
- `blength`: babyâ€™s length at birth (centimeteres)
- `bwt`: babyâ€™s birth weight (grams)
- `delwt`: motherâ€™s weight at delivery (pounds)
- `fincome`: family monthly income (in hundreds, rounded)
- `frace`: fatherâ€™s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto
  Rican, 8 = Other, 9 = Unknown)
- `gaweeks`: gestational age in weeks
- `malform`: presence of malformations that could affect weight (0 =
  absent, 1 = present)
- `menarche`: motherâ€™s age at menarche (years)
- `mheigth`: motherâ€™s height (inches)
- `momage`: motherâ€™s age at delivery (years)
- `mrace`: motherâ€™s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto
  Rican, 8 = Other)
- `parity`: number of live births prior to this pregnancy
- `pnumlbw`: previous number of low birth weight babies
- `pnumgsa`: number of prior small for gestational age babies
- `ppbmi`: motherâ€™s pre-pregnancy BMI
- `ppwt`: motherâ€™s pre-pregnancy weight (pounds)
- `smoken`: average number of cigarettes smoked per day during pregnancy
- wtgain\`: motherâ€™s weight gain during pregnancy (pounds)

#### Data Cleaning

``` r
#clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).
```

#### Regression Model

Propose a regression model for birthweight. \* Model may be based on a
hypothesized structure for the factors that underly birthweight, on a
data-driven model-building process, or a combination of the two. \*
Describe modeling process

- Show a plot of model residuals against fitted values â€“ use
  `add_predictions` and `add_residuals` in making this plot.

#### Compare your model to two others:

- One using length at birth and gestational age as predictors (main
  effects only)

- One using head circumference, length, sex, and all interactions
  (including the three-way interaction) between these

Make this comparison in terms of the cross-validated prediction error;
use `crossv_mc` and functions in `purrr` as appropriate.

Note that although we expect your model to be reasonable, model building
itself is not a main idea of the course and we donâ€™t necessarily expect
your model to be â€œoptimalâ€.
